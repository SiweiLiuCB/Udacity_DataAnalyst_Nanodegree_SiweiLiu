{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Wrangle Report</center>\n",
    "<center>By Siwei Liu</center>\n",
    "<center>Date: June, 4th, 2018</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this data wrangling project, I learned a great deal about data gathering, data assessing and data clean process. The most challenging part for me in this project was the iteration between data assessing and data cleaning. However, after finishing the project, I feel like I am a large step closer to a real Data Analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data gathering process, I gathered data from three different resources. Firstly, I manually downloaded 'twitter-archive-enhanced.csv' file, which is the twitter archive that WeRateDogs gave Udacity exclusive access to. This file contains the basic information of the tweets such as the ID, the text of the tweets, the names of the dogs, etc. Secondly, I programmatically downloaded the dog image prediction file using python's request library, and then stored it in a tsv file. This file contains the neural network predictions of dog breeds. Thirdly, I used the tweet_id in the twitter-archive-enhanced file to query the twitter API and then stored the JSON data to a file called 'tweet_json.txt', with each available tweet's JSON data on its own line. I also created three data frames from these three files, called 'twitter_archive', 'image_prediction' and 'tweet_info', respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I had all these three data frames, I started to assess them. I evaluated the three data frames seperately for their quality issues and tidiness issues, by using '.info()', '.head()' and '.describe()' functions. I also opened the 'twitter-archive-enhanced.csv' in Excel to have a more intuitive feeling of the data. The main quality issues I spotted were missing data, incorrect data, unneeded data, erroneous data types and data inconsistency problems. And the main tidiness issues were 'multiple variables in one column', and 'every type of observational unit' in one data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assessing the data and listing the problems, I started to clean them. I started the cleaning process by making a copy of the original data frames to make sure that the original data frames are kept intact even after my data cleaning and data analysis. I cleaned the problems one by one, on the same order of issues that I identified in my data assessment. The main Pandas functions I used were '.rename()', 'df.merge()', '.drop()', etc. I also used '.isnull()' to check if my cleaning was successful. In addition, I used '.value_counts()' to identify additional problems and iterated back to the data assessing process, after spotting the problem, I cleaned the data again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, through this project, I learned how to quary an API, how to programmatically download data from the websites and how to use various functions in Python Pandas library to clean data. Thanks to the project and the Udacity Data Wrangling courses, I now have a very clear picture of the data gathering, data assessing and data cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
